# BIG-DATA-ANALYSIS
COMPANY: CODTECH IT SOLUTIONS PVT.LTD

*NAME *: M.Bharath kumar Reddy 

*Intern ID *:CT04DZ1927

DOMAIN: Data Analytics

DURATION: 4 WEEEKS

MENTOR: Neela Santhosh Kumar
# I have to enter description of my task
Task Description:
Title: Big Data Analysis using Distributed Computing Frameworks

Objective:
To demonstrate scalability and efficient processing of large datasets using distributed computing frameworks such as PySpark and Dask. The task involves writing a Python script or Jupyter notebook that loads, cleans, processes, and analyzes a large dataset in parallel. The goal is to showcase how these tools can handle datasets too large to fit into memory, utilizing distributed computing to perform efficient data analysis.

Tools & Frameworks Used:

PySpark: Apache Spark's Python API, designed for large-scale data processing. It allows distributed computation across multiple machines, ideal for big data analysis in a cluster environment.

Dask: A flexible parallel computing library that integrates seamlessly with Python. It enables large-scale data processing using parallel computation across multiple cores or machines, ideal for scalable workflows.

Data Processing Tasks:

Data Loading: Import a large CSV dataset and infer its schema.

Data Cleaning: Handle missing values by removing rows with NaN entries to ensure the dataset is ready for analysis.

Data Exploration:

Display basic summary statistics (e.g., count, mean, standard deviation) for numeric columns.

Display the schema and structure of the dataset.

Aggregations: Perform group-by operations to compute summary statistics (e.g., average, min, max, count) on specific columns.

Data Output: Save the cleaned dataset in an optimized format (Parquet) for further processing or analysis.

Expected Outcomes:

The ability to efficiently load and process large datasets (millions of rows) that may not fit into memory.

Demonstration of parallel computation, where tasks like cleaning, aggregating, and summarizing data are distributed across multiple cores/machines.

The generation of insights like basic statistics and aggregations to help understand the distribution and trends within the data.

Deliverable:
A Python script or Jupyter notebook that demonstrates the steps outlined above, with the necessary code to load, clean, analyze, and save the processed data. The script will also include visualizations and/or output tables showcasing the insights derived from the analysis.

Scalability Demonstration:

PySpark will be used to illustrate the scalability of large data processing in a distributed environment, leveraging Spark's ability to handle vast amounts of data in parallel across a cluster.

Dask will be used to showcase how large datasets can be processed using Python's native parallelism without requiring a full cluster, scaling easily from local to cloud-based execution.



# out put screens 
  # PySpark out put
  <img width="1920" height="1046" alt="Image" src="https://github.com/user-attachments/assets/8146e80c-f1c7-4282-8367-54cb8f6034a1" />
  # Dask
  
  <img width="723" height="445" alt="Image" src="https://github.com/user-attachments/assets/183882a9-8611-4bac-87d3-5d9694dcac30" />
